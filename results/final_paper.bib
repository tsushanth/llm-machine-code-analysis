@article{feng2020codebert,
  title={CodeBERT: A pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Deng, Nan and Huang, Xiaocheng and Liu, Bing and Lu, Xiaofu and Qiu, Xipeng and Song, Peng and Tang, Luo and others},
  journal={arXiv preprint arXiv:2002.08155},
  year={2020}
}

@article{guo2020graphcodebert,
  title={GraphCodeBERT: Pre-training code representations with data flow},
  author={Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and others},
  journal={arXiv preprint arXiv:2009.08366},
  year={2020}
}

@inproceedings{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  booktitle={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  booktitle={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{tay2022efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Rao, Jinfeng and Fedus, William and Abnar, Samira and Chung, Hyung Won and Narang, Sharan and Yogatama, Dani and Vaswani, Ashish and Metzler, Donald},
  journal={ACM Computing Surveys},
  volume={55},
  number={6},
  pages={1--28},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@inproceedings{kanade2020learning,
  title={Learning and evaluating contextual embedding of source code},
  author={Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
  booktitle={International Conference on Machine Learning},
  pages={5110--5121},
  year={2020},
  organization={PMLR}
}

@inproceedings{marcelli2021how,
  title={How machine learning is solving the binary function similarity problem},
  author={Marcelli, Andrea and Graziano, Mariano and Ugarte-Pedrero, Xabier and Fratantonio, Yanick and Mansouri, Mohamad and Balzarotti, Davide},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2099--2116},
  year={2021}
}

@article{david2016statistical,
  title={Statistical similarity of binaries},
  author={David, Yaniv and Yahav, Eran},
  journal={ACM SIGPLAN Notices},
  volume={51},
  number={6},
  pages={266--280},
  year={2016},
  publisher={ACM New York, NY}
}

@inproceedings{pearce2022asleep,
  title={Asleep at the keyboard? assessing the security of github copilot's code contributions},
  author={Pearce, Hammond and Ahmad, Baleegh and Tan, Benjamin and Dolan-Gavitt, Brendan and Karri, Ramesh},
  booktitle={2022 IEEE symposium on security and privacy (SP)},
  pages={754--768},
  year={2022},
  organization={IEEE}
}

@misc{openai2023gpt4,
  title={GPT-4 technical report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@software{tiktoken2023,
  title={tiktoken: Fast BPE tokeniser for use with OpenAI's models},
  author={OpenAI},
  url={https://github.com/openai/tiktoken},
  year={2023}
}

@inproceedings{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  booktitle={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{nijkamp2022codegen,
  title={CodeGen: An open large language model for code with multi-turn program synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

@inproceedings{wang2021codet5,
  title={CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={8696--8708},
  year={2021}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}