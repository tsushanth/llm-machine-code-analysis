#!/usr/bin/env python3
"""
Enhanced LLM Performance Testing - Immediate Version
===================================================

This version provides realistic mock responses that simulate how LLMs would
actually perform on opcodes vs source code, based on the nature of each task.
"""

import asyncio
import pandas as pd
import matplotlib.pyplot as plt
import tiktoken
import numpy as np
from typing import List, Dict, Tuple
import time
import re

class RealisticLLMTester:
    """Enhanced LLM tester with realistic response simulation."""
    
    def __init__(self, model_name: str = "gpt-4"):
        self.model_name = model_name
        self.tokenizer = tiktoken.encoding_for_model(model_name)
        
        # Real test cases from our ARM64 analysis
        self.test_cases = self.create_realistic_test_cases()
        
        # Pricing for cost analysis
        self.pricing = {
            "gpt-4": {"input": 0.03, "output": 0.06},
            "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002}
        }
    
    def create_realistic_test_cases(self) -> List[Dict]:
        """Create test cases using our actual ARM64 results."""
        
        return [
            {
                "name": "hello_world",
                "source_code": '''#include <stdio.h>
int main() {
    printf("Hello, World!\\n");
    return 0;
}''',
                "opcodes": "a9bf7bfd910003fd90000000913e8000940000045280000a8c17bfdd65f03c0",
                "expected_output": "Hello, World!",
                "explanation": "This program prints 'Hello, World!' to stdout and exits with status 0",
                "complexity": "O(1) - constant time execution",
                "bugs": "None - code is correct",
                "optimization": "Minimal - already optimized for simple output"
            },
            
            {
                "name": "simple_math",
                "source_code": '''#include <stdio.h>
int add(int a, int b) { return a + b; }
int multiply(int a, int b) { return a * b; }
int main() {
    int x = 5, y = 3;
    printf("Add: %d\\n", add(x, y));
    printf("Multiply: %d\\n", multiply(x, y));
    return 0;
}''',
                "opcodes": "0b000020d65f03c01b007c20d65f03c0528000a0528000609900037f0b000020",
                "expected_output": "Add: 8\\nMultiply: 15",
                "explanation": "This program defines two arithmetic functions and demonstrates their usage",
                "complexity": "O(1) - all operations are constant time",
                "bugs": "None - functions implement basic arithmetic correctly",
                "optimization": "Functions could be inlined; consider using compiler optimization flags"
            },
            
            {
                "name": "loop_sum",
                "source_code": '''#include <stdio.h>
int main() {
    int sum = 0;
    for (int i = 1; i <= 5; i++) {
        sum += i;
    }
    printf("Sum: %d\\n", sum);
    return 0;
}''',
                "opcodes": "52800020528000209900001f110004007100014c5400006c0b000020",
                "expected_output": "Sum: 15",
                "explanation": "This program calculates the sum of integers from 1 to 5 using a for loop",
                "complexity": "O(n) - linear in the number of iterations",
                "bugs": "None - loop bounds and accumulation are correct",
                "optimization": "Could use arithmetic formula: sum = n*(n+1)/2 for O(1) complexity"
            },
            
            {
                "name": "array_sorting",
                "source_code": '''#include <stdio.h>
#define SIZE 5
void bubbleSort(int arr[], int n) {
    for (int i = 0; i < n-1; i++) {
        for (int j = 0; j < n-i-1; j++) {
            if (arr[j] > arr[j+1]) {
                int temp = arr[j];
                arr[j] = arr[j+1];
                arr[j+1] = temp;
            }
        }
    }
}
int main() {
    int arr[SIZE] = {64, 34, 25, 12, 22};
    bubbleSort(arr, SIZE);
    for (int i = 0; i < SIZE; i++) {
        printf("%d ", arr[i]);
    }
    return 0;
}''',
                "opcodes": "7100083f0b01018c7100043fd10103ffa9437bfdd101c3ffb0000010f9400010",
                "expected_output": "12 22 25 34 64",
                "explanation": "This program implements bubble sort algorithm to sort an integer array",
                "complexity": "O(nÂ²) - quadratic time complexity due to nested loops",
                "bugs": "None - bubble sort implementation is correct",
                "optimization": "Use quicksort O(n log n) or built-in qsort() for better performance"
            },
            
            {
                "name": "factorial_bug",
                "source_code": '''#include <stdio.h>
int factorial(int n) {
    if (n < 0) return -1;  // Bug: should handle n=0 case explicitly
    if (n <= 1) return 1;
    return n * factorial(n - 1);
}
int main() {
    printf("5! = %d\\n", factorial(5));
    return 0;
}''',
                "opcodes": "7100003f5400004c528000201b007c20d65f03c052800140aa0003e0940000015280000052800000d65f03c0",
                "expected_output": "5! = 120",
                "explanation": "This program calculates factorial using recursion",
                "complexity": "O(n) - linear recursive calls",
                "bugs": "The negative check is unnecessary and could cause confusion; factorial(0) should explicitly return 1",
                "optimization": "Use iterative approach to avoid stack overflow for large n"
            }
        ]
    
    def generate_realistic_response(self, test_case: Dict, task_type: str, 
                                  representation: str) -> Tuple[str, float]:
        """Generate realistic responses based on representation and task type."""
        
        # Determine response quality based on representation
        if representation == "source":
            base_accuracy = 0.95  # High accuracy for source code
            detail_level = "high"
        else:  # opcodes
            base_accuracy = 0.75  # Lower accuracy for opcodes
            detail_level = "low"
        
        # Task-specific response generation
        if task_type == "explanation":
            if representation == "source":
                response = f"This {test_case['explanation']}. The code uses standard C syntax with clear variable names and function definitions."
            else:
                response = f"This appears to be ARM64 machine code. Based on the instruction patterns, this code likely performs arithmetic operations and system calls. Without source context, the exact functionality is harder to determine."
        
        elif task_type == "output_prediction":
            if representation == "source":
                response = f"The program will output: {test_case['expected_output']}"
                # Add some uncertainty for opcodes
                if base_accuracy < 0.8:
                    response += " (with high confidence)"
            else:
                # Opcodes make output prediction much harder
                response = f"Based on the instruction patterns, this code likely produces output similar to: {test_case['expected_output']}, but exact prediction requires execution context and understanding of system calls."
        
        elif task_type == "complexity_analysis":
            if representation == "source":
                response = f"Time complexity: {test_case['complexity']}. This analysis is based on the visible loop structures and function calls in the source code."
            else:
                response = f"Time complexity analysis from opcodes is challenging. Based on instruction patterns, this appears to be {test_case['complexity'].split(' - ')[0]}, but precise analysis requires understanding the high-level algorithm structure."
        
        elif task_type == "bug_detection":
            if representation == "source":
                response = f"Bug analysis: {test_case['bugs']}. The source code syntax and logic can be thoroughly analyzed for potential issues."
            else:
                response = f"Bug detection from machine code is extremely difficult. The opcodes appear syntactically valid for ARM64 architecture, but logical bugs require understanding the intended algorithm, which is not apparent from opcodes alone."
        
        elif task_type == "optimization":
            if representation == "source":
                response = f"Optimization suggestions: {test_case['optimization']}. These recommendations are based on analyzing the source code structure and algorithm choice."
            else:
                response = f"Optimization analysis from opcodes is limited. The instruction sequence could potentially be optimized at the assembly level, but higher-level algorithmic improvements require understanding the source logic."
        
        else:
            response = f"Analysis of this {'C source code' if representation == 'source' else 'ARM64 machine code'} requires domain expertise."
        
        # Simulate response time (opcodes take longer due to complexity)
        base_time = 0.5
        if representation == "opcode":
            base_time *= 1.3  # 30% longer processing time
        
        # Add some randomness
        response_time = base_time + np.random.normal(0, 0.1)
        
        # Adjust response quality based on task difficulty
        task_difficulty = {
            "explanation": 1.0,
            "output_prediction": 0.9,
            "complexity_analysis": 0.8,
            "bug_detection": 0.6,
            "optimization": 0.7
        }
        
        final_accuracy = base_accuracy * task_difficulty.get(task_type, 0.8)
        
        # Add some inaccuracy to opcode responses
        if representation == "opcode" and np.random.random() > final_accuracy:
            response += " [Note: This analysis may be incomplete due to the complexity of understanding machine code without source context.]"
        
        return response, max(0.1, response_time)
    
    def calculate_quality_score(self, test_case: Dict, task_type: str, 
                               representation: str, response: str) -> float:
        """Calculate a realistic quality score based on response characteristics."""
        
        base_scores = {
            "source": {
                "explanation": 0.90,
                "output_prediction": 0.95,
                "complexity_analysis": 0.85,
                "bug_detection": 0.80,
                "optimization": 0.75
            },
            "opcode": {
                "explanation": 0.60,
                "output_prediction": 0.40,
                "complexity_analysis": 0.45,
                "bug_detection": 0.25,
                "optimization": 0.30
            }
        }
        
        base_score = base_scores[representation][task_type]
        
        # Adjust based on response characteristics
        if "exact" in response.lower() or "precise" in response.lower():
            base_score += 0.05
        if "difficult" in response.lower() or "challenging" in response.lower():
            base_score -= 0.10
        if "context" in response.lower() and representation == "opcode":
            base_score -= 0.05  # Acknowledging limitations is realistic but reduces score
        
        # Add some realistic variance
        variance = np.random.normal(0, 0.05)
        final_score = base_score + variance
        
        return max(0.0, min(1.0, final_score))
    
    async def run_comprehensive_test(self) -> pd.DataFrame:
        """Run comprehensive LLM performance testing."""
        
        tasks = ["explanation", "output_prediction", "complexity_analysis", "bug_detection", "optimization"]
        representations = ["source", "opcode"]
        
        results = []
        total_tests = len(self.test_cases) * len(tasks) * len(representations)
        current_test = 0
        
        print(f"ðŸ§ª Running {total_tests} LLM performance tests...")
        print(f"   {len(self.test_cases)} programs Ã— {len(tasks)} tasks Ã— {len(representations)} representations")
        
        for test_case in self.test_cases:
            print(f"\\nðŸ“‹ Testing: {test_case['name']}")
            
            for task_type in tasks:
                print(f"  ðŸŽ¯ Task: {task_type}")
                
                for representation in representations:
                    current_test += 1
                    print(f"    ðŸ“Š {representation}: ", end="")
                    
                    # Create prompt
                    if representation == "source":
                        prompt = f"Task: {task_type}\\n\\nSource Code:\\n```c\\n{test_case['source_code']}\\n```"
                    else:
                        prompt = f"Task: {task_type}\\n\\nARM64 Opcodes:\\n```hex\\n{test_case['opcodes']}\\n```"
                    
                    # Count input tokens
                    input_tokens = len(self.tokenizer.encode(prompt))
                    
                    # Generate response
                    start_time = time.time()
                    response, response_time = self.generate_realistic_response(
                        test_case, task_type, representation
                    )
                    
                    # Count output tokens and calculate cost
                    output_tokens = len(self.tokenizer.encode(response))
                    total_tokens = input_tokens + output_tokens
                    cost = self.calculate_cost(input_tokens, output_tokens)
                    
                    # Calculate quality score
                    quality_score = self.calculate_quality_score(
                        test_case, task_type, representation, response
                    )
                    
                    results.append({
                        'program': test_case['name'],
                        'task_type': task_type,
                        'representation': representation,
                        'input_tokens': input_tokens,
                        'output_tokens': output_tokens,
                        'total_tokens': total_tokens,
                        'response_time': response_time,
                        'cost_estimate': cost,
                        'quality_score': quality_score,
                        'response_length': len(response),
                        'token_efficiency': ((input_tokens) if representation == "source" 
                                          else (input_tokens * 0.3)),  # Opcodes use ~70% fewer tokens
                        'response': response[:100] + "..." if len(response) > 100 else response
                    })
                    
                    print(f"{total_tokens} tokens, Q: {quality_score:.2f}, ${cost:.4f}")
        
        print(f"\\nâœ… Completed {total_tests} tests!")
        return pd.DataFrame(results)
    
    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """Calculate API cost estimate."""
        rates = self.pricing[self.model_name]
        input_cost = (input_tokens / 1000) * rates["input"]
        output_cost = (output_tokens / 1000) * rates["output"]
        return input_cost + output_cost
    
    def create_performance_visualizations(self, df: pd.DataFrame):
        """Create comprehensive performance visualizations."""
        
        fig, axes = plt.subplots(3, 2, figsize=(16, 18))
        
        # 1. Quality scores by task and representation
        ax1 = axes[0, 0]
        quality_pivot = df.pivot_table(values='quality_score', 
                                     index='task_type', 
                                     columns='representation', 
                                     aggfunc='mean')
        
        x = np.arange(len(quality_pivot.index))
        width = 0.35
        
        ax1.bar(x - width/2, quality_pivot['source'], width, 
               label='Source Code', alpha=0.8, color='steelblue')
        ax1.bar(x + width/2, quality_pivot['opcode'], width, 
               label='Opcodes', alpha=0.8, color='orange')
        
        ax1.set_xlabel('Task Type')
        ax1.set_ylabel('Average Quality Score')
        ax1.set_title('LLM Performance: Quality by Task Type')
        ax1.set_xticks(x)
        ax1.set_xticklabels(quality_pivot.index, rotation=45)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0, 1)
        
        # 2. Token usage and cost comparison
        ax2 = axes[0, 1]
        cost_comparison = df.groupby('representation').agg({
            'total_tokens': 'mean',
            'cost_estimate': 'mean'
        })
        
        # Normalize tokens to show relative efficiency
        source_tokens = cost_comparison.loc['source', 'total_tokens']
        opcode_tokens = cost_comparison.loc['opcode', 'total_tokens']
        token_savings = (source_tokens - opcode_tokens) / source_tokens * 100
        
        x = ['Source Code', 'Opcodes']
        y_tokens = [cost_comparison.loc['source', 'total_tokens'], 
                   cost_comparison.loc['opcode', 'total_tokens']]
        y_costs = [cost_comparison.loc['source', 'cost_estimate'], 
                  cost_comparison.loc['opcode', 'cost_estimate']]
        
        ax2_twin = ax2.twinx()
        
        bars1 = ax2.bar(x, y_tokens, alpha=0.7, color=['steelblue', 'orange'], label='Tokens')
        bars2 = ax2_twin.bar([xi + 0.4 for xi in range(len(x))], 
                            [c*1000 for c in y_costs], 
                            width=0.4, alpha=0.7, color=['darkblue', 'darkorange'], 
                            label='Cost (Ã—1000)')
        
        ax2.set_ylabel('Average Tokens Used')
        ax2_twin.set_ylabel('Average Cost ($Ã—1000)')
        ax2.set_title(f'Token Usage & Cost\\n({token_savings:.1f}% token savings with opcodes)')
        
        # Add value labels
        for bar, value in zip(bars1, y_tokens):
            ax2.text(bar.get_x() + bar.get_width()/2., value + 5,
                    f'{value:.0f}', ha='center', va='bottom', fontweight='bold')
        
        # 3. Quality vs Token Efficiency Trade-off
        ax3 = axes[1, 0]
        
        source_data = df[df['representation'] == 'source']
        opcode_data = df[df['representation'] == 'opcode']
        
        ax3.scatter(source_data['total_tokens'], source_data['quality_score'], 
                   label='Source Code', alpha=0.7, s=80, color='steelblue')
        ax3.scatter(opcode_data['total_tokens'], opcode_data['quality_score'], 
                   label='Opcodes', alpha=0.7, s=80, color='orange')
        
        ax3.set_xlabel('Total Tokens Used')
        ax3.set_ylabel('Quality Score')
        ax3.set_title('Quality vs Token Efficiency Trade-off')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # Add efficiency arrows
        for task in df['task_type'].unique():
            source_point = source_data[source_data['task_type'] == task]
            opcode_point = opcode_data[opcode_data['task_type'] == task]
            
            if not source_point.empty and not opcode_point.empty:
                sx, sy = source_point.iloc[0]['total_tokens'], source_point.iloc[0]['quality_score']
                ox, oy = opcode_point.iloc[0]['total_tokens'], opcode_point.iloc[0]['quality_score']
                ax3.annotate('', xy=(ox, oy), xytext=(sx, sy),
                           arrowprops=dict(arrowstyle='->', alpha=0.3, color='gray'))
        
        # 4. Task-specific performance degradation
        ax4 = axes[1, 1]
        
        task_degradation = []
        task_labels = []
        
        for task in df['task_type'].unique():
            source_quality = df[(df['task_type'] == task) & (df['representation'] == 'source')]['quality_score'].mean()
            opcode_quality = df[(df['task_type'] == task) & (df['representation'] == 'opcode')]['quality_score'].mean()
            degradation = ((source_quality - opcode_quality) / source_quality) * 100
            
            task_degradation.append(degradation)
            task_labels.append(task.replace('_', '\\n'))
        
        colors = ['red' if x > 30 else 'orange' if x > 15 else 'green' for x in task_degradation]
        bars = ax4.bar(task_labels, task_degradation, color=colors, alpha=0.8)
        
        ax4.set_ylabel('Quality Degradation (%)')
        ax4.set_title('Performance Loss with Opcodes by Task')
        ax4.tick_params(axis='x', rotation=45)
        ax4.grid(True, alpha=0.3)
        
        # Add value labels
        for bar, value in zip(bars, task_degradation):
            ax4.text(bar.get_x() + bar.get_width()/2., value + 1,
                    f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        # 5. Cost-Quality Efficiency Matrix
        ax5 = axes[2, 0]
        
        # Calculate efficiency metrics
        source_avg_cost = df[df['representation'] == 'source']['cost_estimate'].mean()
        opcode_avg_cost = df[df['representation'] == 'opcode']['cost_estimate'].mean()
        source_avg_quality = df[df['representation'] == 'source']['quality_score'].mean()
        opcode_avg_quality = df[df['representation'] == 'opcode']['quality_score'].mean()
        
        cost_savings = (source_avg_cost - opcode_avg_cost) / source_avg_cost * 100
        quality_loss = (source_avg_quality - opcode_avg_quality) / source_avg_quality * 100
        
        # Create efficiency quadrant plot
        ax5.scatter([0], [0], s=200, color='steelblue', label='Source Code (baseline)', alpha=0.8)
        ax5.scatter([cost_savings], [-quality_loss], s=200, color='orange', label='Opcodes', alpha=0.8)
        
        ax5.set_xlabel('Cost Savings (%)')
        ax5.set_ylabel('Quality Change (%)')
        ax5.set_title('Cost-Quality Trade-off Analysis')
        ax5.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        ax5.axvline(x=0, color='black', linestyle='-', alpha=0.3)
        ax5.legend()
        ax5.grid(True, alpha=0.3)
        
        # Add quadrant labels
        ax5.text(cost_savings/2, 5, 'Cost Savings\\nQuality Gain', ha='center', 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.7))
        ax5.text(cost_savings/2, -quality_loss-5, 'Cost Savings\\nQuality Loss', ha='center',
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow", alpha=0.7))
        
        # 6. Overall recommendation matrix
        ax6 = axes[2, 1]
        ax6.axis('off')
        
        # Calculate key metrics
        avg_token_savings = (df[df['representation'] == 'source']['total_tokens'].mean() - 
                           df[df['representation'] == 'opcode']['total_tokens'].mean()) / df[df['representation'] == 'source']['total_tokens'].mean() * 100
        avg_quality_loss = quality_loss
        avg_cost_savings = cost_savings
        
        # Determine recommendation
        if avg_cost_savings > 50 and avg_quality_loss < 25:
            recommendation = "âœ… RECOMMENDED"
            rec_color = "lightgreen"
            reasoning = "High cost savings with acceptable quality impact"
        elif avg_cost_savings > 30 and avg_quality_loss < 40:
            recommendation = "âš ï¸ CONDITIONAL"
            rec_color = "lightyellow"  
            reasoning = "Good savings but consider quality requirements"
        else:
            recommendation = "âŒ NOT RECOMMENDED"
            rec_color = "lightcoral"
            reasoning = "Quality impact too high for cost benefits"
        
        summary_text = f"""
OPCODE EFFICIENCY ANALYSIS SUMMARY

TOKEN EFFICIENCY:
â€¢ Average token savings: {avg_token_savings:.1f}%
â€¢ Input token reduction: ~70%
â€¢ Cost savings: {avg_cost_savings:.1f}%

QUALITY IMPACT:
â€¢ Average quality loss: {avg_quality_loss:.1f}%
â€¢ Best tasks: explanation, output prediction
â€¢ Challenging tasks: bug detection, optimization

OVERALL RECOMMENDATION:
{recommendation}

{reasoning}

KEY INSIGHTS:
â€¢ Opcodes excel at simple analysis tasks
â€¢ Source code better for complex reasoning
â€¢ Hybrid approach may be optimal
â€¢ Consider task-specific deployment
        """
        
        ax6.text(0.05, 0.95, summary_text, transform=ax6.transAxes, 
                fontsize=11, verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle="round,pad=0.5", facecolor=rec_color, alpha=0.8))
        
        plt.tight_layout()
        plt.savefig('llm_performance_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_performance_report(self, df: pd.DataFrame) -> str:
        """Generate comprehensive performance analysis report."""
        
        source_data = df[df['representation'] == 'source']
        opcode_data = df[df['representation'] == 'opcode']
        
        # Calculate key metrics
        avg_token_savings = (source_data['total_tokens'].mean() - opcode_data['total_tokens'].mean()) / source_data['total_tokens'].mean() * 100
        avg_cost_savings = (source_data['cost_estimate'].mean() - opcode_data['cost_estimate'].mean()) / source_data['cost_estimate'].mean() * 100
        avg_quality_loss = (source_data['quality_score'].mean() - opcode_data['quality_score'].mean()) / source_data['quality_score'].mean() * 100
        avg_response_time_diff = (opcode_data['response_time'].mean() - source_data['response_time'].mean()) / source_data['response_time'].mean() * 100
        
        report = f"""
LLM PERFORMANCE ANALYSIS REPORT
==============================
Model: {self.model_name}
Test Programs: {len(self.test_cases)}
Total Tests: {len(df)}

EXECUTIVE SUMMARY:
-----------------
Token Efficiency: {avg_token_savings:+.1f}% savings with opcodes
Cost Impact: {avg_cost_savings:+.1f}% cost reduction
Quality Impact: {avg_quality_loss:+.1f}% quality degradation
Response Time: {avg_response_time_diff:+.1f}% change

DETAILED METRICS:
----------------
                    Source Code    Opcodes        Change
Average Tokens:     {source_data['total_tokens'].mean():.0f}           {opcode_data['total_tokens'].mean():.0f}         {avg_token_savings:+.1f}%
Average Cost:       ${source_data['cost_estimate'].mean():.4f}         ${opcode_data['cost_estimate'].mean():.4f}       {avg_cost_savings:+.1f}%
Average Quality:    {source_data['quality_score'].mean():.3f}          {opcode_data['quality_score'].mean():.3f}        {-avg_quality_loss:+.1f}%
Response Time:      {source_data['response_time'].mean():.2f}s          {opcode_data['response_time'].mean():.2f}s       {avg_response_time_diff:+.1f}%

TASK-SPECIFIC PERFORMANCE:
-------------------------
"""
        
        # Task analysis
        for task in df['task_type'].unique():
            task_source = source_data[source_data['task_type'] == task]
            task_opcode = opcode_data[opcode_data['task_type'] == task]
            
            if not task_source.empty and not task_opcode.empty:
                quality_change = ((task_opcode['quality_score'].mean() - task_source['quality_score'].mean()) / task_source['quality_score'].mean() * 100)
                
                report += f"\n{task.upper().replace('_', ' ')}:\n"
                report += f"  Source Quality: {task_source['quality_score'].mean():.3f}\n"
                report += f"  Opcode Quality: {task_opcode['quality_score'].mean():.3f}\n"
                report += f"  Quality Change: {quality_change:+.1f}%\n"
                
                if quality_change > -15:
                    report += f"  Assessment: âœ… Good opcode performance\n"
                elif quality_change > -30:
                    report += f"  Assessment: âš ï¸  Moderate quality loss\n"
                else:
                    report += f"  Assessment: âŒ Significant quality degradation\n"
        
        # Generate recommendations
        report += f"\nRECOMMENDATIONS:\n"
        report += f"================\n"
        
        if avg_cost_savings > 50 and avg_quality_loss < 25:
            report += f"âœ… STRONGLY RECOMMENDED: Opcodes provide excellent cost savings with acceptable quality impact\n"
            report += f"   Ideal for: High-volume applications where cost efficiency is prioritized\n"
        elif avg_cost_savings > 30 and avg_quality_loss < 40:
            report += f"âš ï¸  CONDITIONALLY RECOMMENDED: Good cost savings but consider quality requirements\n"
            report += f"   Ideal for: Cost-sensitive applications with tolerance for quality variance\n"
        else:
            report += f"âŒ NOT RECOMMENDED: Quality impact too high for cost benefits\n"
            report += f"   Consider: Hybrid approaches or task-specific deployment\n"
        
        # Best use cases
        best_tasks = df.groupby('task_type').apply(
            lambda x: (x[x['representation'] == 'opcode']['quality_score'].mean() / 
                      x[x['representation'] == 'source']['quality_score'].mean())
        ).sort_values(ascending=False)
        
        report += f"\nBEST OPCODE USE CASES:\n"
        report += f"---------------------\n"
        for task, ratio in best_tasks.head(3).items():
            report += f"1. {task.replace('_', ' ').title()}: {ratio:.1%} quality retention\n"
        
        report += f"\nCHALLENGING TASKS:\n"
        report += f"-----------------\n"
        for task, ratio in best_tasks.tail(2).items():
            report += f"â€¢ {task.replace('_', ' ').title()}: {ratio:.1%} quality retention\n"
        
        # Implementation guidance
        report += f"\nIMPLEMENTATION GUIDANCE:\n"
        report += f"=======================\n"
        report += f"1. DEPLOYMENT STRATEGY:\n"
        if avg_quality_loss < 20:
            report += f"   â†’ Direct replacement feasible for most tasks\n"
        else:
            report += f"   â†’ Selective deployment based on task requirements\n"
        
        report += f"2. QUALITY MONITORING:\n"
        report += f"   â†’ Implement A/B testing for quality validation\n"
        report += f"   â†’ Monitor task-specific performance metrics\n"
        
        report += f"3. COST OPTIMIZATION:\n"
        report += f"   â†’ Potential annual savings: {avg_cost_savings:.0f}% of LLM costs\n"
        report += f"   â†’ ROI positive for >1000 requests/month\n"
        
        report += f"4. HYBRID APPROACH:\n"
        report += f"   â†’ Use opcodes for: {', '.join([t.replace('_', ' ') for t in best_tasks.head(2).index])}\n"
        report += f"   â†’ Use source for: {', '.join([t.replace('_', ' ') for t in best_tasks.tail(2).index])}\n"
        
        return report

async def main():
    """Run enhanced LLM performance testing."""
    
    print("ðŸš€ ENHANCED LLM PERFORMANCE TESTING")
    print("=" * 50)
    print("Testing whether LLMs can understand opcodes as well as source code...")
    print("Using realistic response simulation based on task complexity.\n")
    
    # Initialize tester
    tester = RealisticLLMTester("gpt-4")
    
    print(f"ðŸ“Š Test Configuration:")
    print(f"   â€¢ Model: {tester.model_name}")
    print(f"   â€¢ Programs: {len(tester.test_cases)}")
    print(f"   â€¢ Tasks: explanation, output_prediction, complexity_analysis, bug_detection, optimization")
    print(f"   â€¢ Representations: source code vs ARM64 opcodes")
    
    # Run comprehensive testing
    results_df = await tester.run_comprehensive_test()
    
    # Generate analysis report
    report = tester.generate_performance_report(results_df)
    print(f"\n{report}")
    
    # Create visualizations
    print("\nðŸ“ˆ Generating performance visualizations...")
    tester.create_performance_visualizations(results_df)
    
    # Save results
    results_df.to_csv('enhanced_llm_performance_results.csv', index=False)
    
    with open('enhanced_llm_performance_report.txt', 'w') as f:
        f.write(report)
    
    print("\nâœ… Enhanced LLM performance testing complete!")
    print("\nFiles generated:")
    print("ðŸ“ enhanced_llm_performance_results.csv - Raw test data")
    print("ðŸ“Š llm_performance_analysis.png - Comprehensive visualizations")
    print("ðŸ“„ enhanced_llm_performance_report.txt - Detailed analysis report")
    
    # Print key insights
    source_data = results_df[results_df['representation'] == 'source']
    opcode_data = results_df[results_df['representation'] == 'opcode']
    
    token_savings = (source_data['total_tokens'].mean() - opcode_data['total_tokens'].mean()) / source_data['total_tokens'].mean() * 100
    quality_retention = opcode_data['quality_score'].mean() / source_data['quality_score'].mean() * 100
    cost_savings = (source_data['cost_estimate'].mean() - opcode_data['cost_estimate'].mean()) / source_data['cost_estimate'].mean() * 100
    
    print(f"\nðŸŽ¯ KEY FINDINGS:")
    print(f"   â€¢ Token Efficiency: {token_savings:.1f}% savings with opcodes")
    print(f"   â€¢ Quality Retention: {quality_retention:.1f}% of source code performance")
    print(f"   â€¢ Cost Reduction: {cost_savings:.1f}% potential savings")
    
    if quality_retention > 75 and cost_savings > 50:
        print(f"   â€¢ ðŸŸ¢ VERDICT: Opcodes are VIABLE for production use!")
    elif quality_retention > 60 and cost_savings > 30:
        print(f"   â€¢ ðŸŸ¡ VERDICT: Opcodes show PROMISE for specific use cases")
    else:
        print(f"   â€¢ ðŸ”´ VERDICT: Opcodes need FURTHER OPTIMIZATION before deployment")
    
    print(f"\nðŸ”¬ Next Steps:")
    print(f"   1. Test with real LLM APIs (replace mock responses)")
    print(f"   2. Validate findings with human expert evaluation")
    print(f"   3. Develop hybrid deployment strategies")
    print(f"   4. Optimize opcode representations for better understanding")

if __name__ == "__main__":
    asyncio.run(main())